{\rtf1\ansi\ansicpg1252\cocoartf2706
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}}{\leveltext\leveltemplateid1\'01\'00;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 K-Nearest Neighbors (KNN)\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b0\fs24 \cf0 K-Nearest Neighbors (KNN) is a simple, non-parametric, lazy learning algorithm used for classification and regression. It works by finding the K nearest data points in the feature space to a given query point and assigning the majority class (in classification) or averaging the values (in regression) from these neighbors to the query point. The primary considerations when using KNN are the choice of K (number of neighbors) and the distance metric (e.g., Euclidean, Manhattan).\
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 K-Means Clustering\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b0\fs24 \cf0 K-Means is an unsupervised clustering algorithm that partitions data into K distinct clusters based on feature similarity. The algorithm works iteratively to assign each data point to one of K clusters by minimizing the variance within each cluster. It involves:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	1	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Initializing K centroids randomly.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	2	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Assigning each data point to the nearest centroid.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	3	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Recomputing the centroids as the mean of all points in the cluster.\
\ls1\ilvl0\kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	4	}\expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Repeating the assignment and update steps until convergence.\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \strokec2 Choosing the number of clusters K is crucial and can be guided by methods like the Elbow method, which looks for a bend in the plot of within-cluster variance versus the number of clusters.\
\pard\pardeftab720\sa280\partightenfactor0

\f0\b\fs28 \cf0 Support Vector Machine (SVM)\
\pard\pardeftab720\sa240\partightenfactor0

\f1\b0\fs24 \cf0 Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It works by finding the hyperplane that best separates the data into classes. The hyperplane is chosen to maximize the margin, which is the distance between the hyperplane and the nearest data points from each class, known as support vectors. SVM can also be extended to handle non-linear boundaries using kernel functions, such as polynomial and radial basis function (RBF) kernels, which map the data into higher dimensions where a linear separation is possible.\
\
\pard\pardeftab720\sa240\partightenfactor0
\cf0 \
}