{\rtf1\ansi\ansicpg1252\cocoartf2706
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\b\fs36 \cf0 \expnd0\expndtw0\kerning0
\ul \ulc0 \outl0\strokewidth0 \strokec2 Artificial Neural Networks (ANNs) using Keras\

\f1\b0\fs24 \ulnone \strokec2 \
\pard\pardeftab720\sa240\partightenfactor0

\f0\b \cf0 \strokec2 Perceptron
\f1\b0 : The chapter begins with the basics of a perceptron, which is the simplest form of a neural network, consisting of a single layer of neurons.\

\f0\b Multilayer Perceptrons (MLP)
\f1\b0 : It introduces MLPs, which are networks with multiple layers of neurons. MLPs are capable of learning more complex patterns than a single-layer perceptron.\

\f0\b Activation Functions
\f1\b0 : Various activation functions, such as ReLU, sigmoid, and tanh, are discussed. These functions determine the output of neurons and introduce non-linearity into the network.\

\f0\b Training Neural Networks
\f1\b0 : The chapter covers how to train neural networks using backpropagation and gradient descent.\

\f0\b Overfitting and Regularization
\f1\b0 : Techniques to prevent overfitting, such as dropout and early stopping, are explained.\

\f0\b Keras
\f1\b0 : The chapter provides a practical introduction to using Keras, a high-level neural networks API, to build, compile, and train neural networks easily.\
}